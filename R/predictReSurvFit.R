#' Predict IBNR frequency
#'
#' This function predicts the results from the ReSurv fits.
#'
#' @param object \code{ResurvFit} object specifying start time, end time and status.
#' @param newdata \code{IndividualDataPP} object that contains new data to predict.
#' @param grouping_method \code{character}, use probability or exposure approach to group from input to output development factors. Choice between:
#' \itemize{
#' \item{\code{"exposure"}}
#' \item{\code{"probability"}}
#' }
#' Default is \code{"exposure"}.
#' @param check_value \code{numeric}, check hazard value on initial granularity, if above threshold we increase granularity to try and adjust the development factor.
#' @param lower_triangular_output \code{logical}, if set to \code{TRUE} we add the predicted lower triangle in input and output granularity to the \code{predict.ReSurvFit} output.
#' @param groups_encoding_output \code{logical}, if set to \code{TRUE} we add a \code{data.table} containing the groups encoding to the \code{predict.ReSurvFit} output.
#' @param ... Additional arguments to pass to the predict function.
#'
#'
#' @return Predictions for the \code{ReSurvFit} model. It includes
#' \itemize{
#' \item{\code{ReSurvFit}: Fitted \code{ReSurv} model.}
#' \item{\code{long_triangle_format_out}: \code{data.frame}. Predicted development factors and IBNR claim counts for each feature combination in long format.}
#' \itemize{
#' \item{\code{input_granularity}: \code{data.frame}. Predictions for each feature combination in long format for \code{input_time_granularity}.}
#' \itemize{
#' \item{\code{AP_i}: Accident period, \code{input_time_granularity}.}
#' \item{\code{DP_i}: Development period, \code{input_time_granularity}.}
#' \item{\code{f_i}: Predicted development factors, \code{input_time_granularity}.}
#' \item{\code{group_i}: Group code, \code{input_time_granularity}. This associates to each feature combination an identifier.}
#' \item{\code{expected_counts}: Expected counts, \code{input_time_granularity}.}
#' \item{\code{IBNR}: Predicted IBNR claim counts, \code{input_time_granularity}.}
#' }
#' \item{\code{output_granularity}: \code{data.frame}. Predictions for each feature combination in long format for \code{output_time_granularity}.}
#' \itemize{
#' \item{\code{AP_o}: Accident period, \code{output_time_granularity}.}
#' \item{\code{DP_o}: Development period, \code{output_time_granularity}.}
#' \item{\code{f_o}: Predicted development factors, \code{output_time_granularity}.}
#' \item{\code{group_o}: Group code, \code{output_time_granularity}. This associates to each feature combination an identifier.}
#' \item{\code{expected_counts}: Expected counts, \code{output_time_granularity}.}
#' \item{\code{IBNR}: Predicted IBNR claim counts, \code{output_time_granularity}.}
#' }
#' }
#' \item{\code{lower_triangle}: Predicted lower triangle.}
#' \itemize{
#' \item{\code{input_granularity}: \code{data.frame}. Predicted lower triangle for \code{input_time_granularity}.}
#' \item{\code{output_granularity}: \code{data.frame}. Predicted lower triangle for \code{output_time_granularity}.}
#' }
#' \item{\code{predicted_counts}: \code{numeric}. Predicted total frequencies.}
#' \item{\code{grouping_method}: \code{character}. Chosen grouping method.}
#'
#' }
#'
#' @importFrom dplyr bind_rows distinct relocate arrange
#' @export
#' @method predict ReSurvFit
predict.ReSurvFit <- function(object,
                              newdata = NULL,
                              grouping_method = "probability",
                              lower_triangular_output = FALSE,
                              minimal_output = FALSE,
                              check_value = 1.85,
                              ...) {


  if (!is.null(newdata)) {

    pkg.env$check.newdata(newdata = newdata,
                          pastdata = object$data_information)


    ## copy the information you need ----

    conversion_factor=newdata$data_information$conversion_factor
    string_formula_i=newdata$data_information$string_formula_i
    string_formula_o=newdata$data_information$string_formula_o
    continuous_features=newdata$data_information$continuous_features
    categorical_features=newdata$data_information$categorical_features
    calendar_period_extrapolation=newdata$data_information$calendar_period_extrapolation
    years=newdata$data_information$years
    accident_period=newdata$data_information$accident_period
    calendar_period=newdata$data_information$calendar_period
    input_time_granularity=newdata$data_information$input_time_granularity
    output_time_granularity=newdata$data_information$output_time_granularity



  } else{
    idata <- object$IndividualDataPP

    ## copy the information you need ----

    conversion_factor=object$data_information$conversion_factor
    string_formula_i=object$data_information$string_formula_i
    string_formula_o=object$data_information$string_formula_o
    continuous_features=object$data_information$continuous_features
    categorical_features=object$data_information$categorical_features
    calendar_period_extrapolation=object$data_information$calendar_period_extrapolation
    years=object$data_information$years
    accident_period=object$data_information$accident_period
    calendar_period=object$data_information$calendar_period
    input_time_granularity=object$data_information$input_time_granularity
    output_time_granularity=object$data_information$output_time_granularity



  }

  # scalar used below (compute once)
  max_dp_i <- pkg.env$maximum.time(
    years,
    input_time_granularity
  )

  # browser()
  is_baseline_model <- is.null(c(categorical_features, continuous_features))

  # Convert to data.table if not already
  hazard_frame <- as.data.table(object$hazard_frame)

  # Drop DP_i
  hazard_frame[, DP_i := NULL]

  # Rename columns
  setnames(hazard_frame, old = c("f_i", "cum_f_i"), new = c("dev_f_i", "cum_dev_f_i"))

  # hazard_frame <- object$hazard_frame %>%
  #   select(-DP_i) %>%
  #   rename(dev_f_i = f_i, cum_dev_f_i = cum_f_i)

  hazard_frame_grouped <- pkg.env$covariate_mapping(
    hazard_frame = hazard_frame,
    categorical_features = categorical_features,
    continuous_features = continuous_features,
    conversion_factor = conversion_factor,
    calendar_period_extrapolation = calendar_period_extrapolation
  )



    # missing.obsevations <- pkg.env$simplified_fill_data_frame(
    #   data = idata$full.data,
    #   continuous_features = continuous_features,
    #   categorical_features =
    #     categorical_features,
    #   years = years,
    #   input_time_granularity =
    #     input_time_granularity,
    #   conversion_factor = conversion_factor
    # )


  # latest_observed computation ----
  ## Retrieve total amount of observed claims as of the evaluation date ----

  data_reserve = object$data_information$data_for_reserving

  max_DP_i <- unique(data_reserve[, .(DP_max_rev = min(max(DP_rev_i) - DP_i) + 1), by = AP_i])

  # Step 1: select relevant columns
  cols_to_keep <- unique(c("AP_i", "AP_o", "DP_rev_i", "DP_i", categorical_features, continuous_features, "I"))
  data_reserve2 <- data_reserve[, ..cols_to_keep]  # .. prefix needed for a vector of names

  # Step 2: convert AP_i to numeric
  data_reserve2[, AP_i := as.numeric(AP_i)]

  # Step 3: left join with max_DP_i by AP_i
  data_reserve2 <- max_DP_i[data_reserve2, on = "AP_i"]

  if(is.null(continuous_features)){

    # Define grouping columns
    group_cols <- c(categorical_features, "AP_i", "AP_o", "DP_max_rev")

    # Compute sum of I by group
    observed_so_far <- data_reserve2[, .(latest_I = sum(I)), by = group_cols]

    # Define grouping columns
    group_cols <- c("AP_i", "AP_o", categorical_features, "DP_rev_i", "DP_i")

    # Summarize I by group
    observed_dp_rev_i <- data_reserve2[, .(I = sum(I)), by = group_cols]

    ## add the group dimension

    # Define feature columns
    feature_cols <- c(categorical_features, continuous_features)

    # Create feature.id efficiently
    observed_so_far[, covariate := do.call(paste, c(.SD, sep = "_")), .SDcols = feature_cols]
    observed_dp_rev_i[, covariate := do.call(paste, c(.SD, sep = "_")), .SDcols = feature_cols]

    # Perform left join
    observed_so_far <- hazard_frame_grouped$groups[observed_so_far, on = "covariate"]

    # Select relevant columns
    observed_so_far <- observed_so_far[, .(AP_i, group_i, DP_max_rev, latest_I)]


    # Perform left join
    observed_dp_rev_i <- hazard_frame_grouped$groups[observed_dp_rev_i, on = "covariate"]

    # Select relevant columns
    observed_dp_rev_i <- observed_dp_rev_i[, .(AP_i, group_i, DP_max_rev, latest_I)]


    out <- pkg.env$latest_observed_values_i(data_reserve,
                                            hazard_frame_grouped$groups,
                                                 categorical_features,
                                            continuous_features,
                                                 FALSE)



  }else{

    # if(length(continuous_features)==1 & "AP_i" %in% continuous_features){
    #
    #   continuous_features_group<-NULL
    #
    # }else{
    #
    #   continuous_features_group <- continuous_features[!(continuous_features %in% c("AP_i","RP_i") )]
    #
    # }

    # if(is.null(continuous_features_group)){
    #
    #   group_cols <- c("AP_i", "AP_o", categorical_features, "DP_max_rev")
    #   # observed_so_far <- data_reserve2[, .(latest_I = sum(I)), by = group_cols]
    #
    #   # Define grouping columns
    #   group_cols <- c("AP_i", "AP_o", categorical_features, "DP_rev_i", "DP_i")
    #
    #   # Summarise
    #   # observed_dp_rev_i <- data_reserve2[, .(I = sum(I)), by = group_cols]
    #
    #
    # }else{
    #
    #   # Define grouping columns
    #   group_cols <- c("AP_i", "AP_o", categorical_features, continuous_features_group, "DP_max_rev")
    #
    #
    #
    #   # Define grouping columns
    #   group_cols <- c("AP_i", "AP_o", categorical_features, continuous_features_group, "DP_rev_i", "DP_i")
    #
    #
    # }


    # Define grouping columns
    group_cols <- unique(c("AP_i", "AP_o", categorical_features, continuous_features, "DP_max_rev"))


    # Summarise
    observed_so_far <- data_reserve2[, .(latest_I = sum(I)), by = group_cols]

    # Define grouping columns
    group_cols <- unique(c("AP_i", "AP_o", categorical_features, continuous_features, "DP_rev_i", "DP_i"))

    # Summarise
    observed_dp_rev_i <- data_reserve2[, .(I = sum(I)), by = group_cols]



    # Define feature columns
    feature_cols <- c(categorical_features, continuous_features[continuous_features!="AP_i"])

    # Create feature.id efficiently
    observed_so_far[, covariate := do.call(paste, c(.SD, sep = "_")), .SDcols = feature_cols]
    observed_dp_rev_i[, covariate := do.call(paste, c(.SD, sep = "_")), .SDcols = feature_cols]


    time_features <- continuous_features[continuous_features %in% c("AP_i","RP_i")]


    # Left join
    latest_cumulative <- hazard_frame_grouped$groups[observed_so_far, on = c(time_features, "covariate")]

    # Select required columns
    latest_cumulative <- latest_cumulative[, c(unique("AP_i", time_features), "group_i", "DP_max_rev", "latest_I"), with = FALSE]


    # Step 1: left join
    observed_dp_rev_i <- hazard_frame_grouped$groups[
      observed_dp_rev_i,
      on = c(time_features, "covariate")
    ]

    # Step 2: keep only required columns
    observed_dp_rev_i <- observed_dp_rev_i[,
                                                   c(unique("AP_i", time_features), "group_i", "DP_rev_i", "DP_i", "I"),
                                                   with = FALSE
    ]

    # Step 3: inner join with (time_features, group_i) from hazard_frame_grouped$groups
    observed_dp_rev_i <- observed_dp_rev_i[
      hazard_frame_grouped$groups[, c(time_features, "group_i"), with = FALSE],
      on = c(time_features, "group_i"),
      nomatch = 0   # ensures inner join
    ]

    # latest_cumulative = observed_so_far_out, observed_pr_dp = observed_dp_rev_i_tmp


    }


  max_DP <- pkg.env$maximum.time(years,
                                 input_time_granularity = output_time_granularity)#max(bind_rows(idata$training.data, object$data_information$missing_data)$DP_rev_o)


  ## Computation of the expected IBNR ----

  grouped_hazard_0 <- latest_cumulative[
    hazard_frame_grouped$hazard_group,
    on = c("group_i", "AP_i")
  ]

  # Step 1: keep DP_max_rev explicitly
  expected_i <- grouped_hazard_0[, .(DP_rev_i, AP_i, group_i, S_i, S_i_lag,
                                   DP_max_rev, latest_I, DP_max_rev_keep = DP_max_rev)]


  # Step 2: hazard_tmp as before
  hazard_tmp <- hazard_frame_grouped$hazard_group[,
                                                  .(DP_rev_i_key = DP_rev_i + 1, AP_i, group_i, S_ultimate_i = S_i)]

  # Step 3: left join (DP_max_rev used only for matching)
  expected_i <- hazard_tmp[expected_i, on = .(DP_rev_i_key = DP_max_rev, AP_i, group_i)]

  # Step 4: fcase now can use DP_max_rev_keep
  ## in this part we DO use the exposure method.
  expected_i[, U := fcase(
    S_i_lag == 1, latest_I,
    DP_max_rev_keep == min(hazard_frame_grouped$hazard_group$DP_rev_i), latest_I,
    S_ultimate_i == 0, 0,
    AP_i != 1, latest_I / S_ultimate_i,
    default = latest_I
  )]


  expected_i_probability <- copy(expected_i)
  expected_i_probability[, I_expected := 1*(S_i_lag - S_i)][, IBNR := fifelse(DP_rev_i < DP_max_rev_keep, I_expected, NA_real_)]

  # Step 5: compute I_expected
  expected_i[, I_expected := U*(S_i_lag - S_i)]

  # Step 6: compute IBNR
  expected_i[, IBNR := fifelse(DP_rev_i < DP_max_rev_keep, I_expected, NA_real_)]





  # Step 7: keep only required columns
  expected_i <- expected_i[, .(AP_i, group_i, DP_rev_i, I_expected, IBNR)]


  hazard_frame_input <- hazard_frame_grouped$hazard_group[
    expected_i,
    on = .(AP_i, group_i, DP_rev_i)
  ]

  hazard_frame_input[,DP_i:=max_dp_i-DP_rev_i +1]



  # In case of output granularity different from input granularity, creating the output -----


  if (conversion_factor != 1 & !(minimal_output)) {





    # left join, derive variables, (optionally filter), then aggregate and select
    expected_o <-expected_i[
        , DP_i := max_dp_i - DP_rev_i + 1
      ][
        , `:=`(
          AP_o     = ceiling(AP_i * conversion_factor),
          DP_rev_o = ceiling(max_dp_i * conversion_factor) -
            ceiling((DP_i + (AP_i - 1) %% (1 / conversion_factor)) * conversion_factor) + 1
        )
        # ][
        #   DP_rev_o > 0  # <- uncomment to reintroduce the filter you commented out in dplyr
      ][
        , .(
          I_expected = sum(I_expected, na.rm = TRUE),
          IBNR       = sum(IBNR,       na.rm = TRUE)
        ),
        by = .(AP_o, DP_rev_o, group_o)
      ][
        , .(AP_o, group_o, DP_rev_o, I_expected, IBNR)
      ]


    hazard_data_frame <- merge(
      hazard_frame_grouped$hazard_group,
      hazard_frame_grouped$groups[, .(group_i, group_o)],
      by = "group_i",
      all.x = TRUE
    )

    observed_pr_dp_o <- merge(
      observed_dp_rev_i,
      hazard_frame_grouped$groups[, .(group_i, group_o)],
      by = "group_i",
      all.x = TRUE
    )

    latest_cumulative_o <- merge(
      latest_cumulative,
      hazard_frame_grouped$groups[, .(group_i, group_o)],
      by = "group_i",
      all.x = TRUE
    )[
      , .(latest_I = sum(latest_I, na.rm = TRUE)),
      by = .(AP_i, group_o, DP_max_rev)
    ]


    expected_i <- merge(
      expected_i_probability,
      hazard_frame_grouped$groups[, .(group_i, group_o)],
      by = "group_i",
      all.x = TRUE
    )


    grouped_hazard_0 <- copy(hazard_frame_grouped$hazard_group)[
      , DP_i := max_dp_i - DP_rev_i + 1
    ][
      , DP_rev_o := floor(max_dp_i * conversion_factor) -
        ceiling(DP_i * conversion_factor +
                  ((AP_i - 1) %% (1 / conversion_factor)) * conversion_factor) + 1
    ][
      DP_rev_o > 0
    ]

    # left join dp_ranges by (AP_i, DP_rev_o)
    grouped_hazard_0 <- merge(
      grouped_hazard_0,
      object$data_information$dp_ranges,
      by = c("AP_i", "DP_rev_o"),
      all.x = TRUE
    )

    grouped_hazard_0[
      hazard_frame_grouped$groups[, .(group_i, group_o)],
      on = "group_i",
      group_o := i.group_o
    ]

    # left join latest_cumulative_o by (group_o, AP_i)
    grouped_hazard_0 <- merge(
      grouped_hazard_0,
      latest_cumulative_o[, .(group_o, AP_i, DP_max_rev, latest_I)],
      by = c("group_o", "AP_i"),
      all.x = TRUE
    )

    # left join observed_pr_dp_o by (group_o, AP_i, DP_rev_i)
    grouped_hazard_0 <- merge(
      grouped_hazard_0,
      observed_pr_dp_o[,.(group_o,AP_i, DP_rev_i, I)],
      by = c("group_o", "AP_i", "DP_rev_i"),
      all.x = TRUE
    )

    # Create cumulative observed to find exposure for each period
    cumulative_observed <- observed_pr_dp_o[
      order(DP_i),                               # arrange by DP_i within groups
      .(exposure  = cumsum(fifelse(is.na(I), 0, I)),   # cumulative sum, treating NA as 0
        DP_rev_i  = DP_rev_i - 1,                      # shift DP_rev_i by -1
        AP_i      = AP_i,
        group_o   = group_o),
      by = .(AP_i, group_o)
    ][, .(AP_i, group_o, DP_rev_i, exposure)]          # select needed columns


    exposures <- grouped_hazard_0[
      , .SD[DP_rev_i == max(DP_rev_i)],
      by = .(AP_i, DP_rev_o, group_o)
    ]

    exposures <- exposures[
      cumulative_observed,
      on = .(AP_i, group_o, max_dp = DP_rev_i),   # max_dp in exposures matches DP_rev_i in cumulative_observed
      exposure := i.exposure
    ]


   # Where we do not have any observed correct exposure we extrapolate based on fitted hazard

    # Step 1 + 2: select and add gm
    no_exposure <- exposures[
      , .(DP_rev_i, DP_rev_o, AP_i, group_o, S_i, DP_max_rev, latest_I)
    ]

    # Step 3: prepare hazard_data_frame for the join
    hazard_tmp <- hazard_data_frame[
      , .(DP_rev_i = DP_rev_i + 1,   # increment by 1
          AP_i,
          group_o,
          S_ultimate_i = S_i)        # rename here
    ]

    # Step 4: left join
    no_exposure <- merge(
      no_exposure,
      hazard_tmp,
      by.x = c("DP_max_rev", "AP_i", "group_o"),
      by.y = c("DP_rev_i",   "AP_i", "group_o"),
      all.x = TRUE
    )

  # grouping method assumed to be probability
  no_exposure[, U := 1 ]

  no_exposure[
    latest_I == 0, U := 0
  ]

  no_exposure[
    , exposure_expected := U * S_i
  ]

  # final subset
  no_exposure <- no_exposure[
    , .(AP_i, group_o, DP_rev_o, DP_rev_i, exposure_expected)
  ]


  # join with no_exposure
  exposures_combined <- merge(
    exposures,
    no_exposure,
    by = c("AP_i", "DP_rev_o", "DP_rev_i", "group_o"),
    all.x = TRUE
  )

  # exposure_combined = coalesce(exposure_expected, 0)
  exposures_combined[
    , exposure_combined := fifelse(!is.na(exposure_expected), exposure_expected, 0)
  ]


  grouped_hazard_1 <- merge(
    grouped_hazard_0,
    expected_i[, .SD, .SDcols = setdiff(colnames(expected_i), c("group_i", "S_i","S_i_lag","latest_I"))],
    by = c("AP_i", "group_o", "DP_rev_i"),
    all.x = TRUE
  )

  # coalesce(I_expected, 0)
  grouped_hazard_1[, I_combined := fifelse(!is.na(I_expected), I_expected, 0)]

  # aggregate
  grouped_hazard_2 <- grouped_hazard_1[
    , .(observed = sum(I_combined, na.rm = TRUE)),
    by = .(AP_i, DP_rev_o, group_o)
  ]

  # join exposures_combined
  grouped_hazard_2 <- merge(
    grouped_hazard_2,
    exposures_combined,
    by = c("AP_i", "group_o", "DP_rev_o"),
    all.x = TRUE
  )

  # overwrite observed if latest_I == 0
  grouped_hazard_2[latest_I == 0, observed := 0]

  output_dev_factor <- grouped_hazard_2[
    , .(dev_f_o = fifelse(
      sum(exposure_combined, na.rm = TRUE) == 0,
      1,
      (sum(observed, na.rm = TRUE) + sum(exposure_combined, na.rm = TRUE)) /
        sum(exposure_combined, na.rm = TRUE)
    )),
    by = .(DP_rev_o, group_o)
  ]

  output_dev_factor[,DP_o:=pkg.env$maximum.time(
    years,
    output_time_granularity
  )-DP_rev_o +1]


  ## 1) Keep only selected features from the first table (plus join keys and useful IDs)
  cols_to_keep <- unique(c("AP_i", "covariate", "group_i",
                           categorical_features, continuous_features))
  cols_to_keep <- intersect(names(hazard_frame_input), cols_to_keep)

  dt1 <- copy(hazard_frame_input)[, ..cols_to_keep]

  ## 2) Merge groups into dt1 by covariate and group_i
  ##    (keeps all rows of dt1, adds AP_o/group_o from groups)
  dtm <- hazard_frame_grouped$groups[
    dt1, on = .(covariate, group_i)
  ]

  ## 3) For each distinct group_o, keep the first row (by AP_i; adjust if you prefer another order)
  setorder(dtm, group_o, AP_i)
  result <- dtm[!is.na(group_o), .SD[1L], by = group_o]

  # Continuous covariates excluding AP_i, group_o, AP_o
  keep_cont <- c(setdiff(continuous_features, c("AP_i")), "group_o", "AP_o")

  # Final set = categorical covariates + filtered continuous covariates
  final_cols <- unique(c(categorical_features, keep_cont))
  final_cols <- intersect(names(result), final_cols)  # guard against typos/missing cols

  final_result <- result[, ..final_cols]

  hazard_frame_output <- expected_o[
    final_result,
    on = .(AP_o, group_o)
  ]

  # Join on DP_rev_o and group_o
  hazard_frame_output <- output_dev_factor[
    hazard_frame_output,
    on = .(DP_rev_o, group_o)
  ]

  # Replace NA dev_f_o with 1
  hazard_frame_output[is.na(dev_f_o), dev_f_o := 1]



  # browser()
  # Final output formatting: no actual computations from here on


  setnames(hazard_frame_input, c("dev_f_i","I_expected"),c("f_i","expected_counts"))
  setnames(hazard_frame_output, c("dev_f_o","I_expected"),c("f_o","expected_counts"))

  hazard_frame_input[,c("expg",
                        "baseline",
                        "hazard",
                        "DP_rev_i"):=NULL]

  hazard_frame_output[,c("DP_rev_o"):=NULL]

  out = list(
    ReSurvFit = object,
    # I removed these two (save memory)
    # df_output = as.data.frame(df_o),
    # df_input = as.data.frame(df_i),
    long_triangle_format_out = list(
      input_granularity = hazard_frame_input,
      output_granularity = hazard_frame_output
    ),
    predicted_counts = sum(hazard_frame_input$IBNR, na.rm = T)
  )



  if (lower_triangular_output) {
    ltr_input <- pkg.env$find_lt_input(hazard_frame_input, max_dp_i)

    setDT(ltr_input)

    ltr_output <- pkg.env$find_lt_output(
      hazard_frame_output,
      max_DP,
      cut_point = ceiling(max_dp_i * conversion_factor)
    )

    setDT(ltr_output)

    ltr_output[,.SD,.SDcols=colnames(ltr_output) %in% as.character(1:max_DP)]

    lower_triangle=list(input_granularity=ltr_input,
                        output_granularity=ltr_output)

    out[['lower_triangle']] <- lower_triangle
  }

  class(out) <- c('ReSurvPredict')

  return(out)
}

  setnames(hazard_frame_input, c("dev_f_i","I_expected"),c("f_i","expected_counts"))
  hazard_frame_input[,c("expg",
                        "baseline",
                        "hazard",
                        "DP_rev_i"):=NULL]

out = list(
  ReSurvFit = object,
  long_triangle_format_out = list(input_granularity = hazard_frame_input),
  predicted_counts = sum(hazard_frame_input$IBNR, na.rm = T)
)

if (lower_triangular_output) {
  ltr_input <- pkg.env$find_lt_input(hazard_frame_input, max_dp_i)

  lower_triangle=list(input_granularity=ltr_input)

  out[['lower_triangle']] <- lower_triangle
}

class(out) <- c('ReSurvPredict')

return(out)

}
